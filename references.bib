@comment{
	Anything here doesn't show up.
	Usual % comments not observed, I don't think
}

@misc{patent,
  title={Overhead Delivery System for Transporting Products},
  author={Brown, A.B. and Fisher, G.R. and Gilman, S.J. and Lang, S.R.},
  url={https://www.google.com/patents/US20170101182},
  year={2017},
  month=apr # "~13",
  publisher={Google Patents},
  note={US Patent App. 14/881,217}
}

@inproceedings{adaptive_cribbage,
	title={Investigation of an adaptive cribbage player},
	author={Kendall, Graham and Shaw, Stephen},
	booktitle={International Conference on Computers and Games},
	pages={29--41},
	year={2002},
	organization={Springer}
}

@phdthesis{evolutionary_learning_game_ai,
	title={Improving adaptive game AI with evolutionary learning},
	author={Ponsen, Marc},
	year={2004},
	school={TUDelft}
}

@article{game_tactics_evol,
	title={Automatically generating game tactics through evolutionary learning},
	author={Ponsen, Marc and Munoz-Avila, H{\'e}ctor and Spronck, Pieter and Aha, David W},
	journal={AI Magazine},
	volume={27},
	number={3},
	pages={75},
	year={2006}
}

@article{adaptive_game_ai_scripting,
	title={Adaptive game AI with dynamic scripting},
	author={Spronck, Pieter and Ponsen, Marc and Sprinkhuizen-Kuyper, Ida and Postma, Eric},
	journal={Machine Learning},
	volume={63},
	number={3},
	pages={217--248},
	year={2006},
	publisher={Springer}
}

@article{poker_challenge,
	title={The challenge of poker},
	author={Billings, Darse and Davidson, Aaron and Schaeffer, Jonathan and Szafron, Duane},
	journal={Artificial Intelligence},
	volume={134},
	number={1-2},
	pages={201--240},
	year={2002},
	publisher={Elsevier}
}

@article{poker_review,
	title={Computer poker: A review},
	author={Rubin, Jonathan and Watson, Ian},
	journal={Artificial Intelligence},
	volume={175},
	number={5-6},
	pages={958--987},
	year={2011},
	publisher={Elsevier}
}

@techreport{roconnor_cs486,
	title={Temporal Difference Reinforcement Learning Applied to Cribbage},
	author={O'Connor, Russell},
	url={http://r6.ca/cs486/},
	institution={University of California, Berkeley},
	year=2000,
	type={Bachelor's Thesis},
	note={Accessed September 5, 2017}
}

@comment{
todo: below is actually a senior thesis, not phd thesis. find a way to fix this
}

@techreport{cribbage_optimal_ev,
	title={Optimal Expected Values for Cribbage Hands},
	author={Martin, Philip L.},
	year={2000},
	institution={Harvey Mudd College},
	type={Bachelor's Thesis},
	url={https://www.math.hmc.edu/seniorthesis/archives/2000/pmartin/pmartin-2000-thesis.pdf},
	note={Accessed May 24, 2017}
}

@online{ACC_rules,
	title = {Rules of Cribbage},
	author = {{American Cribbage Congress}},
	url = {http://www.cribbage.org/rules},
	year = 2018,
	urldate = {2017-05-26},
	note = {Accessed May 26, 2017}
}

@online{ACC_rulebook,
	title = {Official Tournament Rules},
	author = {{American Cribbage Congress}},
	year = 2016,
	url= {http://cribbage.org/rules/ACCRuleBook2016.pdf},
	urldate = {2018-04-09},
	note = {Accessed April 9, 2018}
}

@article{deepmind_alphago,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{deepmind_alphago_zero,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{deepmind_atari,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{deepmind_atari2,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{tdgammon,
  title={Temporal difference learning and {TD-Gammon}},
  author={Tesauro, Gerald},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}

@ARTICLE{uber_nn_plasticity,
  author = {{Miconi}, T. and {Clune}, J. and {Stanley}, K.~O.},
  title = "{Differentiable plasticity: training plastic neural networks with backpropagation}",
  journal = {ArXiv e-prints},
  archivePrefix = "arXiv",
  eprint = {1804.02464},
  keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning, Statistics - Machine Learning},
  year = 2018,
  month = apr,
  adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180402464M},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{why-td-gammon,
  title={Why did {TD-Gammon} work?},
  author={Pollack, Jordan B and Blair, Alan D},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10--16},
  year={1997}
}

@article{samuel_checkers,
  title={Some studies in machine learning using the game of checkers},
  author={Samuel, Arthur L},
  journal={IBM Journal of Research and Development},
  volume={3},
  number={3},
  pages={210--229},
  year={1959},
  publisher={IBM}
}

@unpublished{rl_book,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={2},
  publisher={MIT press Cambridge},
  note={Incomplete draft used available from author's site
	(\url{http://www.incompleteideas.net/book/the-book-2nd.html}).
  	Accessed November 3, 2017}
}

@online{dailycribbagehand,
	BIBITEMLABEL = {DCH},
	title = {{Daily Cribbage Hand}},
	url= {http://www.dailycribbagehand.org},
	urldate = {2018-04-18},
	note = {Accessed April 18, 2018}
}

@techreport{perfinfo,
	title={Infinite games},
	author={Khomskii, Yurii},
	year={2010},
	institution={University of Sofia Bulgaria, Summer Course (July 2010)}
}

@article{stochastic_games,
	author = {Shapley, L. S.},
	title = {Stochastic Games},
	volume = {39},
	number = {10},
	pages = {1095--1100},
	year = {1953},
	doi = {10.1073/pnas.39.10.1095},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/39/10/1095},
	eprint = {http://www.pnas.org/content/39/10/1095.full.pdf},
	journal = {Proceedings of the National Academy of Sciences},
	note = {Accessed May 16, 2018}
}

@online{deepmind_overview,
	author = {{DeepMind}},
	title = {{AlphaGo}},
	year = 2018,
	url = {https://deepmind.com/research/alphago/},
	urldate = {2018-05-18},
	note = {Accessed May 18, 2018}
}

@article{ibm_deepblue,
  title={Deep blue},
  author={Campbell, Murray and Hoane Jr, A Joseph and Hsu, Feng-hsiung},
  journal={Artificial intelligence},
  volume={134},
  number={1-2},
  pages={57--83},
  year={2002},
  publisher={Elsevier}
}

@article{sandholm_poker_improved,
  title={Depth-Limited Solving for Imperfect-Information Games},
  author={Brown, Noam and Sandholm, Tuomas and Amos, Brandon},
  journal={arXiv preprint arXiv:1805.08195},
  year={2018}
}

@article {sandholm_poker,
	author = {Brown, Noam and Sandholm, Tuomas},
	title = {Superhuman AI for heads-up no-limit poker: Libratus beats top professionals},
	year = {2017},
	doi = {10.1126/science.aao1733},
	publisher = {American Association for the Advancement of Science},
	abstract = {No-limit Texas hold{\textquoteright}em is the most popular form of poker. Despite AI successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold{\textquoteright}em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/early/2017/12/15/science.aao1733},
	eprint = {http://science.sciencemag.org/content/early/2017/12/15/science.aao1733.full.pdf},
	journal = {Science},
	note = {Accessed June 12, 2018}
}
