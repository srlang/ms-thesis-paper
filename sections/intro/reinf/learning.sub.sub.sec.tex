\subsubsection*{Learning an Optimal Policy}

%%%
% A walk-through of policy evaluation, policy improvement, and value iteration
%%%

%%%
%For the sake of discussing how an optimal policy can be learned,
%it is useful from hereon to limit the scope of discussion to the case
%of a discrete set of states and actions.
%
Although applicable to both discrete and continuous state representations,
it is useful for the sake of illustration to limit the scope of discussion to
discrete representations.
%
Heretofore, unless otherwise stated, all discussion will assume a discrete
representation.
% TODO: ^^^ check that this is needed
%%%

% TODO: need a value function explanation (background knowledge, maybe?)

\paragraph*{Metrics}

%%%
% Discuss what it means to be optimal/ pi_i > pi_j
% Explanation of value of a state: v_{pi}(s)
% Quality of an action q_{pi}(s,a)
%%%

%%%
A state can have a \textit{value} associated with it given a policy $\policy$.
%
The value of a state $s \in \States$ under policy $\policy$ is denoted
$v_\policy(s)$
which can signal the ``worth'' of the given state
and is defined as the expected total reward by following policy $\policy$ from
state $s$:
\begin{align*}
	v_\policy(s) &= \Expectation_\policy\left[G_t | S_t = s\right] \\
		&= \Expectation_\policy \left[
				\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s
			\right]
\end{align*}
%
The optimum value of a state $v_*(s)$ is defined as:
\[ v_*(s) = \max_\policy v_\policy(s)\ \forall s \in \States \]
%%%

%%%
Similarly,
an action can have its own ``worth'' or \textit{quality} assigned to it under a
specific policy.
%
The quality of an action $a\in\Actions$ at state $s\in\States$ under policy
$\policy$ is defined as:
\begin{align*}
	q_\policy(s,a)
		&= \Expectation_\policy\left[ G_t | S_t = s, A_t = a \right]
		\\
		&= \Expectation_\policy\left[
				\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} | S_t = s, A_t = a
			\right]
\end{align*}
and its optimum $q_*(s,a)$ is accordingly defined as:
\[
	q_*(s,a) = \max_{\policy}q_\policy(s,a)\ 
		\forall a\in\Actions\ \forall s\in\States
\]
%%%

%%%
As previously discussed,
an optimal policy $\policy_*$ is any policy which maximizes the expected reward
received.
%
Furthermore, policies can be compared.
%
A policy $\policy$ is greater than another policy $\policy'$
if and only if the value of all states under policy $\policy$
are greater than or equal to the value of all states under $\policy'$
\[
\policy \ge \policy'\ 
	\text{iff}\ 
	v_\policy(s) \ge v_{\policy'}(s)\ \forall s\in\States
\]
%%%


\paragraph*{Policy Evaluation}

%%%
% What does it mean to evaluate a policy
%%%

%%%
Policy evaluation is the iterative process of approximating the state-value
function $v_\policy$ for some policy $\policy$.
%
By repeatedly updating the value of a state
as its cumulative expected reward when taking each possible action,
the state-value function of a policy can be estimated to an
arbitrary precision.
%
This process is shown in Algorithm~\ref{alg:poleval}.
%
Due to its iterative nature,
however,
this can be a slow process
as each state can affect others in a cascading fashion.
%%%

\input{algs/poleval.alg.tex}

\paragraph*{Policy Improvement and Iteration}

\input{algs/politer.alg.tex}

%%%
After the value of a policy can be accurately estimated,
it is then possible to improve on that policy.
% 
This process is as simple as greedily choosing the action at each state such
that the expected resulting reward is maximized.
%
% TODO: vvv probably reword
Intuitively,
this is understandably a better policy
since each action taken will optimize its expected returns if ever the
agent finds itself in the given state,
leading to a better policy
assuming the starting policy to be improved upon was suboptimal.
%
Using this simple greedy policy improvement mechanism,
the idea of policy iteration is formed.
%
Alternating steps of policy evaluation and policy improvement are repeated
until a stable policy is reached that is approximately optimal.
%
This process is given in Algorithm~\ref{alg:politer}
and can be visualized as:
\[
	\policy_0 \Earrow v_{\policy_0} \Iarrow
	\policy_1 \Earrow v_{\policy_1} \Iarrow
	\policy_2 \Earrow \ldots \Iarrow
	\policy_* \Earrow v_{\policy_*} \Iarrow
\]
where $\Earrow$ shows policy evaluation and $\Iarrow$ show policy improvement.
%
Although Algorithm~\ref{alg:politer}'s conditional check before
returning the value function can allow cycling between multiple optimal
policies,
the algorithm is illustrative
and care can be taken in implementation to remove this possibility.
%%%

	%\subparagraph*{Generalized Policy Iteration}

	%%%
	% A couple small notes about policy iteration in general
	%%%

\paragraph*{Value Iteration}

%%%
% Overview of algorithm and how it's just sweeps of both on occasion
As mentioned,
the iterative nature of policy evaluation leads to a slow converging rate
for the process of policy iteration.
%
However,
the full convergence needs not occur for an optimal policy to be computed.
%
Instead,
a very truncated form of policy evaluation can be done to improve the speed
of evaluation while still converging to an optimal policy.
%
A special case of only performing one pass of policy evaluation and improvement
during policy iteration is called value iteration
and is shown in Algorithm~\ref{alg:valueiter}. 
%
This algorithm still converges to optimal policy,
just with less accurate steps made in improvement.
%
The analogy can be made where value iteration is stochastic gradient descent
to policy iteration's gradient descent.
% TODO: ^^^ probably cite definitions for each or something
%%%

\input{algs/valueiter.alg.tex}

\paragraph*{Monte Carlo Methods}

%%%
% Meaty crux of the thesis.
% An explanation of how policy improvement can be approximated using simulated
% results.
%%%

%%%
Policy iteration requires full knowledge of the environment to be computed.
%
However,
this is often not practical in real-world applications.
%
Monte Carlo methods are a set of ways in which
state-value functions or policies can be learned through experience without
complete knowledge of the environment.
%
These experience can come from episodes in the real environment
or through simulated encounters
where an episode is simply a separable set of events that occur together in
time.
%%%

%%%
By averaging observed returns from experienced episodes,
the state-value function,
and thus policy,
can be learned.
%
The key problem in learning through experience is how to balance
the exploitation of the current value function or policy
with exploration of the environment to discover potentially better returns.
%
The exploration/exploitation debate is analogous to a breadth-first or
depth-first search of a tree.
%
An agent which explores too much (breadth-first)
may not have very accurate knowledge of each subtree,
while an agent which exploits too much (depth-first) will have accurate
knowledge of only a few paths down the tree.
% TODO: ^^^ work analogy better
%
There are two simple mechanisms to ensure the necessary exploration of the
environment.
%
The first,
using exploring starts,
starts each episode randomly in the environment's state-space.
%
An algorithm for this sort of Monte Carlo method is given in
Algorithm~\ref{alg:mces}.
%
The other,
using an $\epsilon$-greedy policy,
follows the policy less strictly,
taking an action at uniform random with chance $\epsilon$.
%
Naturally,
both of these methods can be combined to further ensure adequate exploration.
%%%

\input{algs/mces.alg.tex}
