\subsubsection*{Learning an Optimal Policy}

%%%
% A walk-through of policy evaluation, policy improvement, and value iteration
%%%

%%%
%For the sake of discussing how an optimal policy can be learned,
%it is useful from hereon to limit the scope of discussion to the case
%of a discrete set of states and actions.
%
Although applicable to both discrete and continuous state representations,
as long as it can be represented as a Markov decision process,
it is useful for the sake of illustration to limit the scope of discussion to
discrete representations.
%
Heretofore, %unless otherwise stated,
all discussion will assume a discrete representation
as the domain of cribbage falls into this category
and it streamlines notation.
%%%

\paragraph*{Metrics}

%%%
% Discuss what it means to be optimal/ pi_i > pi_j
% Explanation of value of a state: v_{pi}(s)
% Quality of an action q_{pi}(s,a)
%%%

%%%
A state can have a worth or \textit{value} associated with it,
for a given policy $\policy$.
%
The value of a state $s \in \States$ under policy $\policy$ is denoted
$v_\policy(s)$
and
is defined as the expected total return by following policy $\policy$ from
state $s$:
\begin{align*}
	v_\policy(s) &= \Expectation_\policy\left[G_t \middle| S_t = s\right] \\
		&= \Expectation_\policy \left[
				\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \middle| S_t = s
			\right]
		\\
		&= \sum_{a} {
				\policy(a|s)
				\sum_{s',\,r} {
					P(s',r|s,a) \left[
						r + \gamma v_{\policy}(s')
					\right]
				}
			}
\end{align*}
where $t$ is the arbitrary time within the episode
when the agent is in state $s$,
to maintain compatibility with previous definitions regarding rewards sequences.
%
The optimum value of a state $v_*(s)$ is defined as:
\[ v_*(s) = \max_\policy v_\policy(s) \]
%%%

%%%
Similarly,
an action can have its own worth or \textit{quality} assigned to it under a
specific policy.
%
The quality of an action $a\in\Actions$ at state $s\in\States$ under policy
$\policy$ is defined as:
\begin{align*}
	q_\policy(s,a)
		&= \Expectation_\policy\left[ G_t \middle| S_t = s, A_t = a \right]
		\\
		&= \Expectation_\policy\left[
				\sum_{k=0}^{\infty}\gamma^k R_{t+k+1} \middle| S_t = s, A_t = a
			\right]
\end{align*}
and its optimum $q_*(s,a)$ is consequently defined as:
\[
	q_*(s,a) = \max_{\policy}q_\policy(s,a)
\]
%%%

%%%
As previously discussed,
an optimal policy $\policy_*$ is any policy which maximizes the expected reward
received.
%
If there are optimal and suboptimal policies,
then it follows that there are ways in which policies can be compared.
%
A policy $\policy$ is greater than or equal to another policy $\policy'$
if and only if the value of all states under policy $\policy$
are greater than or equal to the value of all states under $\policy'$
\[
\policy \ge \policy'\ 
	\text{iff}\ 
	v_\policy(s) \ge v_{\policy'}(s)\ \forall s\in\States
\]
%%%


\paragraph*{Policy Evaluation}

%%%
% What does it mean to evaluate a policy
%%%

%%%
Policy evaluation is the iterative process of approximating the state-value
function $v_\policy$ for some policy $\policy$.
%
When the entire state space is observable,
since $v_\policy(s)$ can be expressed recursively,
the value of a state can be repeatedly updated
to its cumulative expected reward when taking each possible action,
converging to the true state-value function.
%
However,
as this convergence only occurs at the limit,
the calculation process can be stopped when its magnitude of change
indicates that it is sufficiently accurate,
as shown in Algorithm~\ref{alg:poleval}.
%
Due to its iterative nature,
however,
this can be a slow process
as each state can affect others in a cascading fashion.
%%%

\input{algs/poleval.alg.tex}

\paragraph*{Policy Improvement and Iteration}

\input{algs/politer.alg.tex}

%%%
After the value of a policy can be accurately estimated,
it is then possible to improve on that policy.
% 
This process is as simple as greedily choosing the action at each state such
that the expected resulting reward is maximized.
%
Intuitively,
this is an optimal policy,
given the current knowledge of the environment,
since each action taken will always make the most rational decision
in whichever state it finds itself in.
%
The idea of policy iteration is formed
from this simple greedy policy improvement mechanism.
%
Alternating steps of policy evaluation and policy improvement are repeated,
converging
until a stable policy is reached that is within some precision bound of optimal.
%
This process is given in Algorithm~\ref{alg:politer}
and can be visualized as:
\[
	\policy_0 \Earrow v_{\policy_0} \Iarrow
	\policy_1 \Earrow v_{\policy_1} \Iarrow
	\policy_2 \Earrow \ldots \Iarrow
	\policy_* \Earrow v_{\policy_*}
\]
where $\Earrow$ shows policy evaluation and $\Iarrow$ shows policy improvement.
%
Although Algorithm~\ref{alg:politer}'s conditional check
in Line 5 of Step 3,
written as is,
can allow cycling between multiple optimal policies,
the algorithm is illustrative
and care can be taken in implementation to remove this possibility.
%%%

	%\subparagraph*{Generalized Policy Iteration}

	%%%
	% A couple small notes about policy iteration in general
	%%%

\paragraph*{Value Iteration}

%%%
% Overview of algorithm and how it's just sweeps of both on occasion
As mentioned,
the iterative nature of policy evaluation leads to a slow convergence rate
for the process of policy iteration.
%
However,
full convergence does not need to occur for an optimal policy to be computed.
%
Instead,
a very truncated form of policy evaluation can be done to improve the speed
of evaluation while still converging to an optimal policy.
%
A special case of only performing one pass of policy evaluation and improvement
during policy iteration is called value iteration
and is shown in Algorithm~\ref{alg:valueiter}. 
%
This algorithm still converges to optimal policy,
just with less accurate steps made in improvement.
%
Rather than make a slow, calculated step in the absolute best direction,
value iteration takes a quick step in a generally good direction,
always improving its situation and
allowing later improvements in judgment to compensate for the potentially poor
steps previously taken.
%%%

\input{algs/valueiter.alg.tex}

\paragraph*{Monte Carlo Methods}

%%%
% Meaty crux of the thesis.
% An explanation of how policy improvement can be approximated using simulated
% results.
%%%

%%%
Policy iteration requires full knowledge of the environment to be computed.
%
However,
this is often not practical in real-world applications.
%
Monte Carlo methods are a set of ways in which
state-value functions, action-value functions, or policies can be estimated through experience without
complete knowledge of the environment.
%
These experiences can come from episodes in the real environment
or through simulated encounters.
%
By averaging observed returns from experienced episodes,
the state-value function,
and thus policy,
can be learned.
%%%

%%%
The key problem in learning through experience is how to balance
exploitation of the current value function or policy
to increase immediate returns
with exploration of the environment to discover potentially better returns.
%
An agent which explores too much
may not have very accurate knowledge of each action's eventual outcome
and often sacrifices better returns in the attempt.
%
Comparatively,
an agent which explores too little
may not discover that a better outcome is possible
than those which it already knows.
%
There are two mechanisms to combat this issue
and ensure the necessary exploration of the environment
while also exploiting current knowledge to maximize rewards.
%
The first,
using exploring starts,
starts each episode randomly in the environment's state-space.
%
An algorithm for this sort of Monte Carlo method is given in
Algorithm~\ref{alg:mces}.
%
This mechanism forces the agent to explore a different region
of the state space than it may likely reach on its own.
%
The other,
using an $\epsilon$-greedy policy,
follows the policy less strictly,
taking an action at uniform random with chance $\epsilon$.
%
Taking an action at random puts the agent in situations
just outside its normal operating area.
%
Whereas random starts can be thought of as global exploration,
random actions would be a local exploration step,
widening the knowledge area around previously known states.
%
Naturally,
both of these methods can be combined to further ensure adequate exploration.
%%%

\input{algs/mces.alg.tex}
