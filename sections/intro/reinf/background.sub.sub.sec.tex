\subsubsection*{Agent, Rewards, and Environment}

%%%
% Explanation of basic background topics in reinforcement learning
%	what is the environment
%	what is reward defined as
%	what is a policy,goal,etc.
%%%

%%%
The three most important, constantly interacting components in a reinforcement
learning scenario are
the environment, the agent, and the rewards.
%
The agent is the actor that makes decisions and learns the task at hand.
%
The agent must learn to navigate the environment in order to maximize its
rewards,
much like a mouse navigating a maze to retrieve the cheese at the end.
%%%

\paragraph*{The Environment}

%%%
In reinforcement learning scenarios,
the agent interacts with and navigates what is known as the environment.
%
The environment is a set of states in which an agent can find itself.
%
What exactly constitutes the environment is problem-specific.
%
An individual state can be any situation in which the agent finds itself
and can be in either discrete or continuous space.
%
For instance, in chess, a discrete state would be a specific board arrangement.
%
In golf, an example of a state in continuous space would be 
the location of the ball along the course of play
and the current wind velocity.
%
An action is an interaction the agent can make with the environment to alter
its current state.
%
In the example of chess,
an action is discrete and would be to move a piece X to position Y,
e.g. moving the bishop to g4 (\textit{Bg4}).
%
In golf, the action is, again, continuous and may be
which club to use in which direction and with how much power.
%%%

\paragraph*{Rewards and Goals}

%%%
Merely being able to move around in an environment does not satisfy the
requirement for learning unless a given task is being completed.
%
This desired task can be called the goal of the agent.
%
For games scenarios,
this is frequently simply the notion of winning.
%%%

%%%
A reward is a positive feedback event that encourages or affirms progress
towards the goal
and 
can be thought of as a way of enticing the agent to accomplish the goal.
%
As with a dog learning to jump through a hoop,
the reward of a treat is given after the dog has successfully jumped
through the hoop,
or perhaps a partial treat for first walking through a stationary hoop
or other similar subtask.
%%%

%%%
Expressed mathematically, a reward $R_t$ is the reward at a given time $t$.
%
A goal $G_t$ is the expected return over time:
\[
	G_t = \sum_{k=t+1}^{T} R_k
\]
where $T$ is the final time step.
%
This goal formula can also incorporate a discounting factor $\gamma$ to
encourage actions conducive to reaching the terminal state in a speedy fashion:
\begin{align*}
	G_t &= \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k \\
		&= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} ... \\
		&= R_{t+1} + \gamma G_{t+1}
\end{align*}
%%%

\paragraph*{Policies}

%%%
A policy is a mapping of actions to states.
%
A policy $\policy$ describes a set of probabilities $P(a|s)$
of taking action $a \in \Actions$
when in state $s \in \States$
where $\Actions$ is the set of all possible actions
and $\States$ is the set of all states in the environment.
%
An optimal policy $\policy_*$,
of which there may be several,
is any policy which achieves a maximum expected reward over the course of
taking its actions.
%%%

