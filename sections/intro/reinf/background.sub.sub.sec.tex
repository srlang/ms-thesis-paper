\subsubsection*{Agent, Rewards, and Environment}

%%%
% Explanation of basic background topics in reinforcement learning
%	what is the environment
%	what is reward defined as
%	what is a policy,goal,etc.
%%%

%%%
The three most important, constantly interacting components in a reinforcement
learning scenario are
the agent, the environment, and the rewards.
%
The agent is the actor that makes decisions and learns the task at hand.
%
The agent must learn to navigate the environment in order to maximize its
rewards,
much like a mouse navigating a maze to retrieve the cheese at the end.
%%%

\paragraph*{The Environment}

%%%
In reinforcement learning scenarios,
the agent interacts with and navigates what is known as the environment.
%
The environment is a set of states in which an agent can find itself.
%
What exactly constitutes the environment is problem-specific.
%
An individual state can be any situation in which the agent finds itself
and can be in either discrete or continuous space.
%
For instance, in chess, a discrete state would be a specific board arrangement.
%
In golf, an example of a state in continuous space would be 
the location of the ball along the course of play
and the current wind velocity.
%
An action is an interaction the agent can make with the environment to alter
its current state.
%
In the example of chess,
an action is discrete and would be to move a piece X to position Y,
e.g.\  moving the bishop to g4 (\textit{Bg4}).
%
In golf, the action is continuous and may be
which club to use in which direction and with how much power.
%%%

\paragraph*{Rewards and Goals}

%%%
Merely being able to navigate an environment does not satisfy the
requirement for learning unless a given task is being completed.
%
This desired task can be called the goal of the agent.
%
For games scenarios,
this is frequently simply the notion of winning.
%%%

%%%
A reward is a feedback event that encourages or affirms progress
towards the goal
and 
can be thought of as a way of enticing the agent to accomplish the task.
%
As with a dog learning to jump through a hoop,
the reward of a treat is given after the dog has successfully jumped
through the hoop,
or perhaps a partial treat for first walking through a stationary hoop
or other similar subtask.
%
Rewards can also be negative
and thought of as punishments for not completing the task,
or doing so in an undesired manner.
%
The goal of the agent is to maximize its cumulative received reward
in the future.
%%%

%%%
Expressed mathematically, $R_t$ is the reward at a given time $t$.
%
A return, $G_t$, is the total reward expected to be received by the agent
in the future:
\[
	G_t = \sum_{k=t+1}^{T} R_k
\]
where $T$ is the final time step.
%
This return formula can also incorporate a discounting factor $\gamma$ to
encourage actions conducive to reaching the terminal state in a speedy fashion:
\begin{align*}
	G_t &= \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k \\
		&= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots \\
		&= R_{t+1} + \gamma G_{t+1}
\end{align*}
%
A rational agent will take an action to maximize its expected return.
%%%

%%%
Consider the scenario of trying to escape a maze.
%
The reward for the agent may be +1 when it successfully exits the maze
and 0 at all other times.
%
Without a discounting factor in the returns,
there would be no incentive to exit the maze in a timely fashion
as the return remains the same no matter how long the agent takes.
%
To handle this without a discount factor,
the rewards would need to be negative for all non-exit spaces
so that the return is worse the longer the agent remains in the maze.
%%%

%%%%
%In the context of cribbage,
%a reasonable reward formulation,
%and the one used in this thesis,
%would be the final point spread observed by the agent after playing the game.
%%
%Therefore,
%the agent's goal at any given state is to win
%by as many points as possible to increase the reward obtained,
%as quickly as possible to reduce the decay observed.
%%%%

\paragraph*{Policies}

%%%
A policy is a mapping of actions to states.
%
A policy $\policy$ describes a set of probabilities $P(a|s)$
of taking action $a \in \Actions$
when in state $s \in \States$
where $\Actions$ is the set of all possible actions
and $\States$ is the set of all states in the environment.
%
An optimal policy $\policy_*$,
of which there may be several,
is any policy which achieves a maximum expected reward over the course of
taking its actions.
%%%

