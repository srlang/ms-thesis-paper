% Discussion of how we learned policy which didn't have enough information
% and how value function improvement would have been better

\subsection{Learning Policy vs. State-Value Function}
\label{sec:disc-value}

%%%
This thesis focused on an attempt to use
$\epsilon$-greedy Monte Carlo methods with exploring starts
to develop a well-playing
cribbage agent capable of weighing different strategies
with the intention of using these learned weights to 
potentially
improve the play of the author and readers.
%
In the primary objective of evolving a cribbage agent to a good strategy,
despite the setbacks of poor performance on a per-game level,
the agent did remarkably well
by removing irrelevant strategies
such as \handmaxmed\ and \peggingmaxmedgained\ 
and emphasizing more useful strategies
like \handmaxavg\ and \handmaxmin.
%%%

%%%
One of the reasons for its poor per-game play
is the lack of knowledge as to how a set of cards lends itself to being played
with a certain strategy.
%
The extent to which the cards themselves affect the action choice in the policy
was greatly underestimated.
%
However,
to include the knowledge of which cards the agent has been dealt into the policy decision
would have increased the dimensionality of the search space from the relatively
small three-dimensional problem tackled to a problem of at least nine
dimensions,
depending on the cards' encoding method.
%
At this dimensionality,
the search space would be far too massive and sparse to simulate an amount of
games which would lead to any worthwhile learning
in a manageable amount of time.
%%%

%%%
An alternative to including the cards in the policy would be to not
directly use a policy at all.
%
Instead,
the state-value function could be used,
as done with TD-Gammon~\cite{tdgammon},
AlphaGo~\cite{deepmind_alphago,deepmind_alphago_zero},
and O'Connor's senior project~\cite{roconnor_cs486}.
%
By learning the state-value function instead of a policy,
an agent could determine which set of cards would likely place itself in the
most optimal resulting position,
indirectly developing its own policy.
%
This form of value iteration should allow for a more adaptive agent
and thus more optimal gameplay,
leading to an agent that should win more consistently.
%%%

%\input{sections/discussion/value/*}
