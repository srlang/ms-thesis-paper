
%%%
% Regularization to prevent too strong of weights
%%%

\subsubsection*{Regularization}

%%%
% Process
%%%

%%%
As an attempt to prevent a single strategy's weight from being so strongly 
preferred that a second strategy could not hope to possibly gain ground,
a hard limit was placed on the pre-normalized update value.
%
Expressed mathematically,
\[
    \wvecm'_{p,o,d}[i] = \max\{K,c\wvecm_{p,o,d}[i]\}
\]
where $K$ is some constant value throughout the training.
%
While the value of $\wvecm_{p,o,d}[i]$ could exceed $K$ after re-normalization
for a particularly strongly weighted strategy,
the value could be seen converging to $K$ within just a handful of iterations.
%
The desired intention of this regularization was to allow other strategies
the opportunity to overcome the bias of earlier strengthening of the strongest
strategy.
%%%

\paragraph*{Results}

%%%
% Results:
%	Similar areas taken up for hand_max_{min,avg}
%		both share same space rather than Croatia/Boz&Herz-shape
%	Grayer, since values only so large
%	As max value is increased,
%		darker, more like un-regulated
%	Islands still present
%	losing, still uncertain
%		still chance of higher than regulation, because of aforementioned reason
%%%

\input{sections/findings/experiments/regularization/strats-0.50.fig.tex}
%\input{sections/findings/experiments/regularization/strats-0.60.fig.tex}
%\input{sections/findings/experiments/regularization/strats-0.70.fig.tex}
\input{sections/findings/experiments/regularization/strats-0.80.fig.tex}
\input{sections/findings/experiments/regularization/strats-comp.fig.tex}

%%%
The limitation of maximum attainable value did indeed force the agent to learn
multiple applicable strategies for each score location.
%
As can be seen in Figure~\ref{fig:reg-strats-0.50},
the \handmaxmin\ and \handmaxavg\ strategies are now 
more-or-less equally represented across the winning states.
%
This is demonstrated by the monotone gray seen in these locations.
%
Rather than each strategy specializing to its own dominant location,
the territory is shared between the two most applicable strategies.
%
Therefore,
the regularization does indeed prevent the dominance of a single strategy
unintentionally discarding all other potential strategies.
%%%

%%%
Additionally,
the nature of this gray mass alters significantly as the regularization rate is
altered.
%
With a lower allowed maximum value,
the strategy graphs take on the previously mentioned slightly amorphous, gray
blob shape.
%
As the maximum value approaches one,
the gray shape begins to specialize more and begins to show resemblance to the
final strategy graphs obtained without regularization.
%
A visualization of this process can be found in
Figure~\ref{fig:expts-reg-comp}.
%%%

\input{sections/findings/experiments/regularization/tourny-regs.fig.tex}

%%%
Furthermore,
as can be seen in Figure~\ref{fig:reg-tournies},
only a regularization rate of $r = 0.80$ (Figure~\ref{fig:reg-tournies-0.80})
can be said to be similar to a loss curve.
%
With rates of $r=0.50$ (Figure~\ref{fig:reg-tournies-0.50}),
$r = 0.60$ (Figure~\ref{fig:reg-tournies-0.60}),
and $r = 0.70$ (Figure~\ref{fig:reg-tournies-0.70}),
the tournament point spread curves show that a trained agent plays on par with
its recent checkpoints,
but consistently worse than random weights.
%
The endpoints of the point spread curves using $r = 0.80$
show a decrease from better-than-random play for the trained agent
to more-or-less on-par performance with its later checkpoints,
indicative of an increase in performance as training progresses.
%
However,
the increases and decreases of performance along the time axis,
the wide range of spreads present,
and the sinusoidal nature of intermediate checkpoints' spreads
prevents the conclusion that performance is progressively increasing.
%%%

