
%%%
% Discussion of how each strategy was used as a separate starting point
%%%

\subsubsection*{Policy Initialization}
\label{sec:findings-expts-starts}

%%%
Since a desired outcome of the learning process was to be able to use the
generated strategy graphs to tell how a hand should be played in a
certain score position,
a comparison was made between the produced agent and pure strategies
on a database of choices made by humans.
%
The website Daily Cribbage Hand
prompts its users with
a set of dealt cards,
their score,
their opponent's score,
and an indication of whether or not the player is the dealer for the round
\cite{dailycribbagehand}.
%
With this prompt,
the user then decides which cards they would
keep in the given scenario.
%%%

%%%
With access to recorded answers from this website,
the agent's choices could be compared
to how humans ranked the choice.
%to those made by humans in the same situation.
%
For each of the approximately 3600 usable records,
the choice the agent made was compared against those made by the users of the
website.
%%%

%%%
The results of this comparison,
seen in Table~\ref{tab:expts-starts-human}, % TODO: ref more tables if created
show that the agent trained in Round 2's losers' bracket chooses the same
set of cards as the human users only marginally more often than an agent with 
randomly allocated weights.
%
To approximately half of the prompts,
the trained agent chooses the same answer as most humans;
in almost 78 percent of the situations,
the answer given by the agent is within the top three most common human answers.
%
Additionally,
most pure strategies,
created by setting their weight to 1 while all other strategies are set to 0,
performed worse than the trained agent.
%
Notable exceptions to this trend are the \handmaxposs\ and \handmaxavg\ 
strategies,
suggesting that in more situations than the agent,
the typical human player will play according to what points can be expected
to be gained from the cut card.
%
Interestingly,
the \handmaxposs\ strategy's presence as the second most common pure strategy
used indicates a significant degree of risk-taking present in the users'
responses.
%
According to user comments,
this increased riskiness in play
may be a result of some users attempting to maximize points of the hand,
without the responsibility to actually play the resulting game,
and may not be entirely representative of actual in-game choices.
%%%

%%%
As a result of this finding,
some of these strategies were used as initial weights to the learning process
in order to determine if the agent could learn to fine-tune a policy starting
from a reasonable assertion of good game-play
as well as learn to discount demonstrably poor strategies.
%
Since the update mechanism for weights relies upon renormalization of a vector
which as been rewarded or punished,
no modifications would occur in the case of punishment of a completely pure strategy
since no other weights would have the chance to increase.
%
Therefore,
the pure strategies used before were slightly modified
such that each other
element of the \wvec\ vector would have a small initial value which could be
increased when this semi-pure strategy was punished.
%
Two agents,
each initialized with the same semi-pure strategy,
were played against each other for a million games,
with only a single agent updating its weights.
%%%

\input{sections/findings/experiments/starting_points/human-comp.tab.tex}


\paragraph*{Results}

%%%%
%There are very two interesting trends that arise from starting from nearly pure
%strategies.
%%
%The first is that the starting strategy is,
%at least for those tested,
%the dominant strategy in winning positions.
%%
%The second is what learning occurs in losing positions.
%%%%

%%%
The comparison of different strategies' development after training
(Figure~\ref{fig:findings-expts-sanitycheck-matrix})
shows the agent learning applicable policies
for both winning and losing positions.
%
In the winning positions of the strategy graphs,
the starting strategy is further strengthened to dominance.
%
The purely mathematical operation of overcoming such an initially heavily
weighted strategy requires many games to explore enough
and perform better than the starting strategy
to be rewarded;
%
the explored combination of cards, itself,
may coincide with the starting strategy
already occupying these positions,
further increasing the difficulty.
%
Additionally,
since the opponent is a static agent always using the same starting
semi-pure strategy and never training these weights,
playing a similar strategy ensures a similar resulting position.
%
When already in the lead,
this is desirable as the agent will likely continue to be in a winning position
and closer to the goal score of 121.
%
An intriguing counter to this pattern of dominance in winning positions
is the \handmaxavg\ strategy
which yields some control of winning positions to \handmaxmed.
%
After normal training procedures,
this space is occupied by \handmaxmin,
which, characteristically, would not allow a larger gap to develop.
%not allowing a larger gap to develop.
%%%

%%%
In losing positions,
the agent develops a reasonable counter policy
to the given semi-pure opponent.
%
For instance,
when faced with \handmaxmin\ which will always play safe,
the agent learns to swing for the fences by playing according to \handmaxposs\ 
when it is losing
since the opponent will not be taking any risks itself
and risk is the only way to make up ground.
%
Similarly,
the agent learns that the best way to recover from a losing position
against \handmaxavg,
which plays to expectations and avoids unnecessary risk,
is to mostly play safely according to \handmaxmin.
%
As a counter to the dominance of a single strategy in the losing positions,
starting with \handmaxposs\ leads to a losing strategy
shared between \handmaxavg\ and \handmaxmed.
%
This is because either of these two strategies is likely to recover ground
against an agent which always tries to get the maximum amount of points
with no regard for the likelihood of this outcome.
%%%

% fig:findings-expts-sanitycheck-matrix
\input{sections/findings/experiments/starting_points/strat-matrix.fig.tex}

%%%
As this experiment showed promise in an agent potentially learning
to out-play its opponent,
an agent trained with a beginning pure strategy of \handmaxavg\ was played
against its previous iterations in several 10,000-game tournaments.
%
The results,
depicted in Figure~\ref{fig:expts-sanitycheck-spreads}
show a steady decrease in performance as training proceeds.
%
Earlier iterations,
closer to the semi-pure \handmaxavg\ strategy,
perform better than the trained agents.
%
Without further context,
it would appear that \handmaxavg\ is simply a good strategy which is difficult
enough to overcome,
so the training framework is forcing an adaptation
unnecessarily and undesirably.
%
However,
as the strategy graphs developed against a static semi-pure \handmaxavg\ 
opponent match those developed against a random agent
(Figure~\ref{fig:expts-sanitycheck-strats}),
this conclusion is unfounded.
%
If the \handmaxavg\ strategy were clearly superior to others,
then very little modification would be made by the agent.
%
Instead,
the losing states still drifted away from the starting strategy.
%
The only conclusions that can be made from this data are that
losing positions are fundamentally difficult to recover from
and that
cribbage involves much more than is encapsulated by the small set of
strategies with which the agent has been programmed.
%%%

% fig:expts-sanitycheck-spreads[-{a,b}]
\input{sections/findings/experiments/starting_points/tourny.fig.tex}

%%%%%%
%%%Furthermore,
%%%as the agents were previously each trained against different pure strategies,
%%%the ability to improve a basic policy to overcome a randomly-weighted agent,
%%%rather than to learn a policy from scratch,
%%%was tested by training an agent starting with a semi-pure
%%%\handmaxavg\ strategy against an unlearning agent with random weights.
%%%%
%%%The results of this further training,
%%%seen in Figure~\ref{fig:expts-sanitycheck-strats},
%%%show that the same behavioral trends are learned by an
%%%agent starting with a semi-pure \handmaxavg\ strategy 
%%%when the opponent is random
%%%as well as when the opponent follows the same semi-pure strategy.
%%%%%%

% fig:expts-sanitycheck-strats
\input{sections/findings/experiments/starting_points/strats.fig.tex}

