
%%%
% Discussion of how each strategy was used as a separate starting point
%%%

\subsubsection*{Starting Points}
\label{sec:findings-expts-starts}

%%%
Since a desired outcome of the learning process was to be able to use the
generated strategy graphs to tell how a hand \textit{should} be played in a
certain score position,
a comparison was made between the produced agent and pure strategies
on a database of choices made by humans.
%
There exists a website in which users are prompted with a set of dealt cards and
a given score and must decide which set of cards they would keep in that
specific situation
\cite{dailycribbagehand}.
%%%

%%%
With access to recorded answers,
the agent's choices could be compared to how humans ranked the choice.
%
The retrieved database recorded which responses were given to each query
and could be used to determine how well the agent's choice matched with those
made by humans in the same situation.
%
For each of the more than 3600 usable records,
the choice the agent made was compared against those made by the users of the
website.
%
The results of this comparison can be seen in
Table~\ref{tab:expts-starts-human}. % TODO: ref more tables if created
%%%

%%%
Table~\ref{tab:expts-starts-human} shows that the trained agent chooses the same
set of cards as the human users only marginally more often than an agent with 
randomly allocated weights.
%
In approximately half of the cases,
the trained agent chooses the same answer as most humans;
in almost 78 percent of the cases,
the answer given by the agent is within the top three most common human answers.
%
Additionally,
most pure strategies,
created by setting their weight to 1 while all others are 0,
performed worse than the trained mixture.
%
Notable exceptions to this trend are the \handmaxposs\ and \handmaxavg\ 
strategies,
suggesting that in more situations than the agent,
the typical human player will play more according to what points can be expected
to be gained from the cut card.
%
Interestingly,
the \handmaxposs\ strategy's presence as the second most common pure strategy
used indicates a significant degree of risk-taking present in the users'
responses.
%%%

%%%
As a result of this finding,
each of these strategies were used as initial weights to the learning process
in order to determine if the agent could learn to fine-tune a policy starting
from a reasonable assertion of good game-play
as well as learn to discount demonstrably poor strategies.
%
Since the update mechanism for weights relies upon renormalization of a vector
which as been rewarded or punished,
no modifications would occur in the case of punishment of a pure strategy
since no other weights would have the chance to increase.
%
Therefore,
the pure strategies used before were slightly modified so that each other
element of the \wvec\ vector would have a small initial value which would be
increased when the pure strategy was punished.
%%%

\input{sections/findings/experiments/starting_points/human-comp.tab.tex}


\paragraph*{Results}

%%%
There are very two interesting trends that arise from starting from nearly pure
strategies.
%
The first is that the starting strategy is,
at least for those tested,
is the dominant strategy in winning positions.
%
Similarly,
this strategy is always un-learned in losing positions.
%
This serves as a confirmation that losing positions are simply unlikely to be
recovered from by the agent,
no matter how seemingly rational the strategy is.
%
There are two likely reasons for this dominance of the winning positions'
weights.
%
The first,
the pure mathematical operation of overcoming such an astoundingly heavily
weighted strategy requires many games to explore enough and perform better than
the given strategy
which itself may often coincide with \handmaxavg\ or \handmaxmin\ 
which would normally occupy these positions.
%
The second,
since the opponent is a static agent always using the same starting
mostly pure strategy and never training these weights,
playing a similar strategy ensures a similar resulting position.
%
When already in the lead,
this desirable as the agent will likely continue to be in a winning position
and closer to the goal score of 121.
%
An intriguing counter to this pattern of dominance in winning positions
is the \handmaxavg\ strategy
which yields some control of winning positions to \handmaxmed.
%
In normal training procedures,
this space is occupied by \handmaxmin.
%%%

%%%
The second interesting observation is what learning does occur
in the losing positions.
%
With most starting strategies,
the losing positions are mainly left to the influence of a single
other strategy.
%
This losing strategy also varies depending on the starting strategy.
%
The reason for this varying losing strategy is again
because the opponent for each of these agents during training
was an agent always using the same mostly pure strategy.
%
As a result,
the learning agent developed a counter strategy to each different
opponent it faced.
%
For instance,
when faced with \handmaxmin\ which will always play safe,
the agent learns to swing for the fences by playing according to \handmaxposs\ 
when it is losing
since the opponent will not be taking any risks itself
and risk is the only way to make up ground.
%
Similarly,
the agent learns that the best way to recover from a losing position
against \handmaxavg,
which plays to expectations and avoids unnecessary risk,
is to mostly play safely according to \handmaxmin.
%
As a counter to the dominance of a single strategy in the losing positions,
starting with \handmaxposs\ leads to a losing strategy
shared between \handmaxavg\ and \handmaxmed.
%
This is because either of these two strategies is likely to recover ground
against an agent which always tries to get the maximum amount of points
with no regard for the likelihood of this outcome.
%%%

% fig:findings-expts-sanitycheck-matrix
\input{sections/findings/experiments/starting_points/strat-matrix.fig.tex}

%%%
As this experiment showed promise in an agent potentially learning
to out-play its opponent,
an agent trained with a beginning pure strategy of \handmaxavg\ was played
against its previous iterations in several 10,000-game tournaments.
%
% TODO: vvv sanity check results
The results,
depicted in Figure~\ref{fig:expts-sanitycheck-spreads}
show that ... TODO ...
%%%

% fig:expts-sanitycheck-spreads[-{a,b}]
\input{sections/findings/experiments/starting_points/tourny.fig.tex}

%%%
% TODO: vvv sanity check second training round results
Furthermore,
as the agents were previously each trained against different pure strategies,
the ability to improve a basic policy to overcome a randomly-weighted agent,
rather than to learn a policy from scratch,
was tested by training an agent starting with a semi-pure
\handmaxavg\ strategy against an unlearning agent with random weights.
%
The results of this further training,
seen in Figure~\ref{fig:expts-sanitycheck-strats},
show that ... TODO ...
%%%

% fig:expts-sanitycheck-strats
\input{sections/findings/experiments/starting_points/strats.fig.tex}

