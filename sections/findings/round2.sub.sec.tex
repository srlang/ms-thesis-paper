% Round 2 findings section

\subsubsection*{Round 2}
\label{sec:findings-r2}

%%%
Round consisted of 
%%%

\paragraph*{Performance}
\label{sec:findings-r2-perf}

%%%
Judging by the lack of pattern found in
Figures~\ref{fig:r2-spreads-winner},
\ref{fig:r2-spreads-loser},
and~\ref{fig:r2-spreads-random},
there is no significant performance increase to be found from Round 2.
%
If learning had occurred,
Figures~\ref{fig:r2-spreads-winner-a},
\ref{fig:r2-spreads-loser-a},
and~\ref{fig:r2-spreads-random-a}
would take on the form of a decreasing curve,
gradually approaching zero,
while Figures~\ref{fig:r2-spreads-winner-b},
\ref{fig:r2-spreads-loser-b},
and~\ref{fig:r2-spreads-random-b}
would be their mirror across the x-axis.
%
Figure~\ref{fig:r2-spreads-winner} alone
would indicate that an agent trained for one million games plays on par with
one trained for two million games.
%
On its own,
this would indicate that performance had reached a peak and saturated.
%
However,
the similarity to Figure~\ref{fig:r2-spreads-loser}
and even to Figure~\ref{fig:r2-spreads-random}
shows that neither bracket learned to perform better than previous iterations.
%%%

\input{sections/findings/round2/spreads_winner.fig.tex}

\input{sections/findings/round2/spreads_loser.fig.tex}

\input{sections/findings/round2/spreads_random.fig.tex}



\paragraph*{Learning Process and Results}
\label{sec:findings-r2-results}

%%%
% Discussion of what it learns and why that's interesting from a cribbage
% perspective
%%%

\input{sections/findings/round2/strategies_winner.fig.tex}

%%%
Despite the lack of performance increase after another million games played by
both the winning and losing brackets,
there are still interesting trends to be spotted between the different brackets
of play.
%
The most notable is how quickly policies converge to a similar state.
%
While the strategy graphs for the less trained agents are less ``crisp''
in their appearance thanks to their slower learning rates,
the patterns of which policy to mainly follow at which times
are still quick to form.
%%%

%%%
Even more fascinating observations can be found from the strategy graphs
from the winner's bracket (see Figure~\ref{fig:r2-strats-winner}).
%
By focusing on the \handmaxmin\ and \handmaxavg\ strategies in particular,
the sinusoidal wave along the diagonal can be observed extending further
back along the diagonal to earlier game positions,
albeit with smaller amplitude.
%
While not being of much use to the agent directly,
this is a useful observation from a cribbage player's perspective.
%
Since it is possible to infer the state-value function,
the implications of this wave are that earlier states are not crucial
predictors for future success.
%
Not only that,
but the agent has learned that changes in lead are likely and not necessarily
detrimental to the ability to win the game.
%%%

%%%
Care needs to be taken with this previous observation on the wave's amplitude
and it must be pointed out to be speculation on the part of the author.
%
It may be the case that with enough training the sine wave will be at full
amplitude in earlier positions.
%
A reminder must be made that
a notable reason for the lack of clarity at the moment is the weight adjustment
mechanism for training.
%
The training system pre-supposes that the earlier positions are of less
importance in a game and adjusts them with a lower priority than later positions
in the game.
%
It may indeed be the case that two million games is still not enough to
counteract the decay of temporal difference learning with the learning
rate used.
%%%


%%%
% Talking about loser's bracket and how it compares to winner's
%%%

\input{sections/findings/round2/strategies_loser.fig.tex}

%%%
By comparing the winner's bracket strategy graphs
(Figure~\ref{fig:r2-strats-winner})
to those of the loser's bracket
(Figure~\ref{fig:r2-strats-loser}),
a vast degree of similarity can be found.
%
In both brackets,
the same behavioral trends are learned.
%
However,
there does exist a small amount of difference between the two
in how quickly and surely each trend is learned.
%
The winner's bracket mostly reinforces its current weight choices
with the losing positions becoming only slightly gradiented.
%
Meanwhile,
in the loser's bracket,
the gradient can be seen slightly more evenly to all strategies
and
across the winning-losing boundary.
%
Of additional note,
there are fewer spaces in which
strategies such as \cribminavg, \peggingmaxavggained, and \peggingminavggiven\ 
have been strengthened within the \handmaxmin\ block.
%
This is a good indicator that,
although present,
there may be less of a bias towards those strategies which are initially winners.
%%%

% TODO: vvv is this used? let's find out
%\input{sections/findings/round2/strategies_random.fig.tex}

%%%
% TODO: discuss random tourny's stuff
%%%

%%%
In analyzing the evolution of the \handmaxavg\ strategy in both the winner's
(Figure~\ref{fig:r2-flip-winner})
and loser's brackets (Figure~\ref{fig:r2-flip-loser}),
it can clearly be seen that the general trend of behavior forms
relatively quickly,
i.e. after approximately 500,000 games played.
%
Beyond that amount of games,
the agent learns to refine the strategy boundaries,
but it can be seen that there is little further discovery being made.
%%%


\input{sections/findings/round2/flipbook_loser.fig.tex}

\input{sections/findings/round2/flipbook_winner.fig.tex}

% TODO: vvv make sure unused
%\input{sections/findings/round2/flipbook_random.fig.tex}


%%%
% Section on why the hell nothing seems to work (speculation)
%%%

\input{sections/findings/round2/short-v-long.sub.sub.sec.tex}

