% Round 2 findings section

\subsubsection*{Round 2}
\label{sec:findings-r2}

%%%
Round 2 consisted of ten agents paired off against each other
in both the winners' and losers' brackets.
%
The winners' bracket used agents previously trained from Round 1
while the losers' bracket agents started with random initializations.
%
These agents were each trained for a million games
with mostly the same configuration as in Round 1,
except the scaling factor was lowered to $s = 2.0$
and exploration rate was set to a constant $e = 0.30$.
%%%

\paragraph*{Performance}
\label{sec:findings-r2-perf}

%%%
Judging by the lack of pattern found in
Figures~\ref{fig:r2-spreads-winner}
and~\ref{fig:r2-spreads-loser},
%and~\ref{fig:r2-spreads-random},
there is no significant performance increase to be found from Round 2.
%
If learning had occurred,
Figures~\ref{fig:r2-spreads-winner-a}
and~\ref{fig:r2-spreads-loser-a}
%and~\ref{fig:r2-spreads-random-a}
would take on the form of a decreasing curve,
gradually approaching zero,
while Figures~\ref{fig:r2-spreads-winner-b}
and~\ref{fig:r2-spreads-loser-b}
%and~\ref{fig:r2-spreads-random-b}
would be their translation across the x-axis.
%
On its own, Figure~\ref{fig:r2-spreads-winner}
would indicate that an agent trained for one million games plays on par with
one trained for two million games.
%
Alone,
this would indicate that performance had reached a peak and saturated.
%
However,
the similarity to Figure~\ref{fig:r2-spreads-loser}
%and even to Figure~\ref{fig:r2-spreads-random}
shows that neither bracket learned to perform better than previous iterations.
%%%

\input{sections/findings/round2/spreads_winner.fig.tex}

\input{sections/findings/round2/spreads_loser.fig.tex}

%\input{sections/findings/round2/spreads_random.fig.tex}



\paragraph*{Learning Process and Results}
\label{sec:findings-r2-results}

%%%
% Discussion of what it learns and why that's interesting from a cribbage
% perspective
%%%

\input{sections/findings/round2/strategies_winner.fig.tex}

%%%
Despite the lack of performance increase after another million games played by
both the winners' and losers' brackets,
there are still interesting trends to be spotted between the different brackets
of play.
%
The most notable is how quickly policies converge to a similar state.
%
While the strategy graphs for the less trained agents are less crisp
in their appearance thanks to their slower learning rates,
the patterns of which policy to mainly follow at which times
are still quick to form.
%%%

%%%
Even more fascinating observations can be found from the strategy graphs
from the winners' bracket (see Figure~\ref{fig:r2-strats-winner}).
%
By focusing on the \handmaxmin\ and \handmaxavg\ strategies in particular,
the sinusoidal wave along the diagonal can be observed extending further
back along the diagonal to earlier game positions,
albeit with smaller amplitude.
%
While not being of much use to the agent directly,
this is a useful observation from a cribbage player's perspective.
%
Since it is possible to infer the state-value function,
the implications of this wave are that earlier states are not crucial
predictors for future success.
%
Not only that,
but the agent has learned that changes in lead are likely in later game states
and not necessarily detrimental to the ability to win the game.
%%%

%%%%
%Care needs to be taken with this previous observation on the wave's amplitude
%and it must be pointed out to be speculation on the part of the author.
%%
%It may be the case that with enough training the sine wave will be at full
%amplitude in earlier positions.
%%
%A reminder must be made that
%a notable reason for the lack of clarity at the moment is the weight adjustment
%mechanism for training.
%%
%The training system pre-supposes that the earlier positions are of less
%importance in a game and adjusts them with a lower priority than later positions
%in the game.
%%
%It may indeed be the case that two million games is still not enough to
%counteract the decay of temporal difference learning with the learning
%rate used.
%%%%


%%%
% Talking about losers' bracket and how it compares to winner's
%%%

\input{sections/findings/round2/strategies_loser.fig.tex}

%%%
By comparing the winners' bracket strategy graphs
(Figure~\ref{fig:r2-strats-winner})
to those of the losers' bracket
(Figure~\ref{fig:r2-strats-loser}),
a vast degree of similarity can be found.
%
In both brackets,
the same behavioral trends are learned.
%
However,
there does exist a small amount of difference between the two
in how quickly and surely each trend is learned.
%
The winners' bracket mostly further strengthens its current weight choices
with the losing positions becoming only slightly gradiented.
%
Meanwhile,
in the losers' bracket,
the gradient can be seen applied slightly more evenly to all strategies
and
across the winning-losing boundary.
%
Of additional note,
there are fewer spaces in which
strategies such as \cribminavg, \peggingmaxavggained, and \peggingminavggiven\ 
have been strengthened within the \handmaxmin\ block.
%
This is a good indicator that,
although present,
there may be less of a bias towards those strategies which are initially winners.
%%%


%%%
In analyzing the evolution of the \handmaxavg\ strategy in both the winners'
(Figure~\ref{fig:r2-flip-winner})
and losers' brackets (Figure~\ref{fig:r2-flip-loser}),
it can clearly be seen that the general trend of behavior forms
relatively quickly,
i.e.\  after approximately 500,000 games played.
%
Beyond that amount of games,
the agent learns to refine the strategy boundaries,
but it can be seen that there is little further discovery being made.
%%%


\input{sections/findings/round2/flipbook_loser.fig.tex}

\input{sections/findings/round2/flipbook_winner.fig.tex}



%%%
The pairing of a learning agent with a non-learning, randomly weighted agent
provided no additional benefit.
%
The same behavioral trends seen in the losers' bracket are echoed
in Figure~\ref{r2-strats-random}.
%
The lack of clarity found in the images is the result of
fewer games being played because of batch job scheduling issues.
%
From these images,
it becomes clear that the learning mechanism,
as is,
is not conducive to playing a better game of cribbage.
%
Thus there needs to be some form of adjustment made in the framework.
%%%

\input{sections/findings/round2/strategies_random.fig.tex}


%%%
% Section on why the hell nothing seems to work (speculation)
%%%

\input{sections/findings/round2/short-v-long.sub.sub.sec.tex}

