
\subsection{Training}
\label{sec:dm-methods-training}

%%%
% How the agent was trained with expert witnesses, etc.
%	mention also the need for half-baked back propagation learning because
%	what is "good" is very relative
%		if it were concrete, only one strategy would be learned for the whole
%		game. opposite of desired outcome
%%%

%%%
% Introductory remarks
% TODO
%%%

%%%
% Expert Witness Testimony functionality
% TODO
%%%

%%%
% Reinforcement Learning
%%%

%%%
After a complete game had been played, the winning and losing agents need to
modify their weights in order to decide a ``correct'' strategy at that time
coordinate.
%
In contrast with textbook forms of reinforcement learning,
the agent is not directly learning which cards to take and throw.
%
To do so would require a search space of ${6 \choose 4}{52 \choose 6}$
at each state.
%
Instead, the agent is learning which subset of strategies make the best
decision in combination at a given point.
%
Therefore, there is no deterministic single action taken at any given
time.
%
Instead, the strategies which advocated most for the chosen hand would be held
responsible for the success or failure of the given hand and thought of as
the action at the given step.
%
The strategies which most advocated for the hand were determined by those whose
values in $S$ were the highest in its column.
% TODO: ^^^ check column/row, put in indices
%
This subset of strategies,
carefully chosen to only include the outliers or top percentage of contributors
to avoid resulting uniform weights,
would then be adjusted by a percentage of itself according to the formula:
\[
	w_{i,new} = c w_{i,old}
\]
for all indices $i$ which are in the most-highly-advocated group and
where $c$ is a shared adjustment constant.
%
All other weights would be left alone before re-normalizing the weights vector.
%
Since locations earlier in the game have a higher number of potential resulting
states and are less likely to affect the final outcome of the game,
they are modified less than their later counterparts.
%
This is accomplished by decaying the adjustment amount for each step taken
backward along the visited path of states:
\[
	c_j = C \cdot (1 - d)^{T-j}
\]
where $j$ is the index along the path starting at 1,
$C$ is a starting value of the adjustment constant,
$T$ is the final step index,
and $d$ is the rate of decay expressed as a ratio such that $0 < d < 1$.
%%%

%%%
The resulting effect is to slightly reward or punish the weights
corresponding to only the strategies which were most in favor of choosing
the chosen cards.
%
Note that this is not necessarily the same set of weights which were the highest
contributors to the final choice.
%
It may be the case that weights $x$,and $y$ are the highest at a given point,
but that strategies $u$ and $v$ with slightly lower weight values had higher
advocacy for a given hand which summed up to a larger value.
%
Were this not the case and instead the highest weights were to be directly
punished or rewarded instead,
without looking at their position on the current hand,
it is more likely that a single group of weights would simply cycle
back and forth in value, never yielding control to other strategies.
%%%

%%%
% TODO
% How exploration steps worked
%%%

%%%
The reinforcement training framework operated very simply.
%
Given an initialization weights file for each agent, a new agent would be
created with those stored weights.
%
These agents would then be placed into a game and played against each other.
%
After the game had been completed, the weights for each agent would be adjusted
accordingly along the path which the agent took.
%
After a set number of epochs, the agent would save its weight-state to a
checkpoint file.
%
This allowed not only for tracking of weight adjustments over time, but also
allowed for the ability to more easily recover potential issues and for more
promising states to be explored further by using a checkpoint as an
initialization state on a subsequent training run.
%
This checkpointing system allowed agents to be mixed and matched together to
allow different combinations to learn from one another.
%%%

%%%
% TODO: mention randomized initializations
%
To facilitate an adequate rate of exploration of the search space,
randomized initializations were used for each of the training games.
%
A random score was chosen for each of the agents,
keeping the spread of scores to within 60 points
in order to limit the search space to only imaginably likely reachable
situations.
%
This value was chosen since,
with a maximum point total of 121,
it is highly unlikely to get into a situation where one player is half the board
behind the other.
%
While there have been anecdotes of losses by more than that amount,
it is a very rare occurrence and can be thought of as a failure of luck
rather than of skill.
%
As this is a matter of training skill and proficiency in the game of cribbage,
situations of exceptional bad luck can be treated as an outlier situation.
%%%

%\DeclareMathOperator{\variance}{
\newcommand{\Var}{\mathrm{Var}}
%%%
A further measure was taken to ensure adequate exploration.
%
Under normal conditions,
cards were chosen by using the produced $p$ vector as a probability
distribution.
%
At any given point in a training game, however,
there was a chance of randomly selecting which cards are played,
with the percent chance $e$ of random choice related to the variance of the 
weights according to the formula:
\[
	e = k - \Var(w_{m,o,d})
\]
where $k$ is a constant around $\frac{1}{3}$ empirically chosen during the
development process.
%
This was implemented in order to ensure that situations in which there were
more uniform weights had a higher chance of being explored
whereas those with a higher variance were deemed to be varied enough to be
exploratory in nature.
%
Furthermore, this acted as yet another barrier to the possibility of falling
into the pitfall of stationary uniform weights.
%%%

%%%
After a set number of training games,
in the case of this project one million,
the two agents were played against each other in a tournament match fashion.
%
The winner was determined as the agent with the most points at the end
of a series of games, %\textemdash
in this case 101, %\textemdash
wherein two points would be awarded to the winner of a game and three if that
win was by a margin of at least 31 points
and ties broken by total point spread.
%
After the match had completed,
the winner would advance to the next round,
training against another winner from the previous round as the process is
repeated until one agent is declared the ultimate winner.
%%%

