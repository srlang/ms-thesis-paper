
\subsection{Training}
\label{sec:dm-methods-training}

%%%
% How the agent was trained with expert witnesses, etc.
%	mention also the need for half-baked back propagation learning because
%	what is "good" is very relative
%		if it were concrete, only one strategy would be learned for the whole
%		game. opposite of desired outcome
%%%

%%%
% Reinforcement Learning
%%%

%%%
After a complete game has been played, the winning and losing agents need to
modify their weights in order to determine a correct strategy at that time
coordinate.
%
The agent attempted to learn which subset of strategies make the best
decision in tandem at a given point.
%
Therefore, there is no deterministic single action taken at any given
time.
%
Instead, the strategies which advocated most for the chosen hand would be held
responsible for the success or failure of the given hand and thought of as
the action at the given step.
%
These responsible strategies were determined as those whose
values in \Smat\ were the highest in its column.
%
This subset of strategies,
carefully chosen to include only the top contributors
to avoid stagnant resulting weights,
was then adjusted proportionally to itself according to the formula:
\[
	\wvecm'_{i} = c \wvecm_{i}
\]
for all indices $i$ which are in the responsible group and
where $c$ is a shared adjustment constant.
%
All other weights would be left unaltered
before the updated vector was re-normalized to the $L^1$-norm.
%%%

%%%
The resulting effect of this mechanism is to slightly reward or punish the weights
corresponding to only the strategies which were most in favor of choosing
the chosen cards.
%
Note that this is not necessarily the same set of weights which were the highest
contributors to the final choice.
%
It may be the case that weights $x$ and $y$ are the highest
at a given score location,
but that strategies $u$ and $v$,
with lower weight values,
had higher advocacy for a given hand which summed up to a larger value.
%
Were this not the case,
and, instead,
the highest weights were to be directly punished or rewarded,
the resulting changes would merely reward or punish those strategies which
were randomly allocated slightly higher weights initially.
%%%

%%%
Since locations earlier in the game have a higher number of potential resulting
states and are less likely to affect the final outcome of the game,
they are modified less than their later counterparts.
%
This is accomplished by decaying the adjustment amount for each step taken
backward along the visited path of states in manner similar to TD($\lambda$)
\cite{tdgammon}:
\[
	c_j = C \cdot (1 - d)^{T-j}
\]
where $j$ is the index along the path starting at 1,
$C$ is a starting value of the adjustment constant,
$T$ is the final step index,
and $d$ is the rate of decay expressed as a ratio such that $0 \leq d \leq 1$.
%
In order to allow for closer losses to be less severely punished than worse
losses and vice versa for wins,
the adjustment constant $C$ was defined as proportional to the difference
between the players' scores according to the formula:
\[
	C = s \cdot \left({PlayerScore} - {OpponentScore}\right) 
\]
where $s$ is a scaling factor,
analogous to a learning rate parameter.
%
This scaling factor remains constant throughout training
and does not itself decrease or decay.
%
This is because all games are equally important,
so a loss using strategy $i$ during game $x$ is just as important as
it would be during game $y$.
%%%

%%%
The reinforcement training framework operated very simply.
%
A text file of weights was loaded to initialize an agent.
%
Two agents were then placed into a game and played against each other.
%
After the game had been completed,
the weights for each agent were adjusted
accordingly along the path taken by the agent.
%
After a set number of epochs, the agent saved its weights configuration to a
checkpoint file.
%
This allowed for the convenient benefit of being able to track how weights
adjusted over the course of time.
%%%

%%%
To facilitate an adequate rate of exploration of the state space,
randomized initializations were used for each of the training games.
%
A random score was chosen for each of the agents,
keeping the spread of scores to within 60 points
in order to limit the search space.
%
This value was chosen since,
with a maximum point total of 121,
it is highly unlikely to get into a situation where one player is half the board
behind the other.
%
While there have been anecdotes of losses by more than that amount,
it is a very rare occurrence and can be thought of as a failure of luck
rather than of skill.
%
As this is a matter of training skill and proficiency in the game of cribbage,
situations of exceptional bad luck can be treated as outlier situations.
%%%

%\DeclareMathOperator{\variance}{
\newcommand{\Var}{\mathrm{Var}}
%%%
A further measure was taken to ensure adequate exploration.
%
Under normal conditions,
cards were selected by choosing the combination which had the highest value in
the produced \pvec\ vector.
%
At any given point in a training game, however,
there was a chance of selecting which cards are played
by random by using \pvec\ as a probability distribution.
%
This chance $e$ of random choice was related to the variance of the 
weights according to the formula:
\[
	e = k - \Var(\wvecm_{m,o,d})
\]
where $k$ is set at a constant $0.3$ empirically chosen during the
development process.
%
This was implemented in order to ensure that situations in which there were
more uniform weights had a higher chance of being explored,
whereas those with a higher variance were deemed to be varied enough to
have been previously trained,
and thus should be further exploited.
%%%

%%%
The training was intended to take the form of a tournament of learning in which
better and better agents would be paired off against each other until an
ultimate agent was reached,
allowing for more experience to be gained for agents from a variety of sources.
%
After a set number of training games,
one million in the case of this thesis, 
the two agents were played against each other in a tournament match fashion.
%
The winner was determined as the agent with the most points at the end
of a series of games, %\textemdash
wherein two points would be awarded to the winner of a game
or three if that win was by a margin of at least 31 points.
%
Ties were broken by total point spread,
according to ACC rules~\cite{ACC_rules}.
%
After the match had completed,
the winner advanced to the next round,
training against another winner from the previous round.
%
The process would be
repeated until one agent is declared the ultimate winner.
%%%

%%%
Very quickly,
the tournament structure was found to not provide any further benefits
to the learning process.
%
As a result,
a set of additional experiments were performed to determine the limits of
learning possible.
%
While these will be covered in more detail in Section~\ref{sec:findings},
these modifications included simple
parameter adjustments, such as to the learning or decay rate,
as well as
more complicated alterations 
including using neighboring weights
or punishing a loss less severely than a corresponding win is rewarded.
%%%

