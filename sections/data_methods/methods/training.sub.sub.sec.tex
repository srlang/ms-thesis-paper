
\subsection{Training}
\label{sec:dm-methods-training}

%%%
% How the agent was trained with expert witnesses, etc.
%	mention also the need for half-baked back propagation learning because
%	what is "good" is very relative
%		if it were concrete, only one strategy would be learned for the whole
%		game. opposite of desired outcome
%%%

%%%
% Introductory remarks
% TODO
%%%

%%%
% Reinforcement Learning
%%%

%%%
After a complete game has been played, the winning and losing agents need to
modify their weights in order to decide a ``correct'' strategy at that time
coordinate.
%
In contrast with textbook forms of reinforcement learning,
the agent is not directly learning which cards to take and throw.
%
To do so would require a search space of ${6 \choose 4}{52 \choose 6}$
at each state.
%
Instead, the agent is learning which subset of strategies make the best
decision in combination at a given point.
%
Therefore, there is no deterministic single action taken at any given
time.
%
Instead, the strategies which advocated most for the chosen hand would be held
responsible for the success or failure of the given hand and thought of as
the action at the given step.
%
The strategies which most advocated for the hand were determined as those whose
values in \Smat\ were the highest in its column.
% TODO: ^^^ check column/row, put in indices
%
This subset of strategies,
carefully chosen to only include the top percentage of contributors
to avoid stagnant resulting weights,
would then be adjusted by a percentage of itself according to the formula:
\[
	\wvecm_{i,new} = c \wvecm_{i,old}
\]
for all indices $i$ which are in the most-highly-advocated group and
where $c$ is a shared adjustment constant.
%
All other weights would be left alone before re-normalizing the weights vector.
%
Since locations earlier in the game have a higher number of potential resulting
states and are less likely to affect the final outcome of the game,
they are modified less than their later counterparts.
%
This is accomplished by decaying the adjustment amount for each step taken
backward along the visited path of states in manner similar to TD($\lambda$):
\[
	c_j = C \cdot (1 - d)^{T-j}
\]
where $j$ is the index along the path starting at 1,
$C$ is a starting value of the adjustment constant,
$T$ is the final step index,
and $d$ is the rate of decay expressed as a ratio such that $0 < d < 1$
\cite{tdgammon}.
%
In order to allow for closer losses to be less severely punished than worse
losses and vice versa for wins,
the adjustment constant $C$ was defined as proportional to the difference
between the players' scores according to the formula:
\[
	C = s \cdot \left({MyScore} - {OppScore}\right) 
\]
where $s$ is a scaling factor analogous to a learning rate parameter.
%
This scaling factor remains constant throughout training
and does not decrease as it would in something like gradient descent.
%
This is because all games are equally important,
so a loss using strategy $i$ during game $x$ is just as important as
it would be during game $y$.
%%%

%%%
The resulting effect is to slightly reward or punish the weights
corresponding to only the strategies which were most in favor of choosing
the chosen cards.
%
Note that this is not necessarily the same set of weights which were the highest
contributors to the final choice.
%
It may be the case that weights $x$ and $y$ are the highest at a given point,
but that strategies $u$ and $v$ with slightly lower weight values had higher
advocacy for a given hand which summed up to a larger value.
%
% TODO(Chelsea): vvv reword
Were this not the case and instead the highest weights were to be directly
punished or rewarded instead,
without looking at their position on the current hand,
it is more likely that a single group of weights would simply cycle
back and forth in value,
never allowing other strategies to be explored.
%%%

%%%
The reinforcement training framework operated very simply.
%
A simple text file of weights can be loaded to initialize an agent.
%
These agents would then be placed into a game and played against each other.
%
After the game had been completed, the weights for each agent would be adjusted
accordingly along the path which the agent took.
%
After a set number of epochs, the agent would save its weights configuration to a
checkpoint file.
%
This allowed for the convenient benefit of being able to track how weights
adjusted over the course of time.
%%%

%%%
To facilitate an adequate rate of exploration of the search space,
randomized initializations were used for each of the training games.
%
A random score was chosen for each of the agents,
keeping the spread of scores to within 60 points
in order to limit the search space to only imaginably likely reachable
situations.
%
This value was chosen since,
with a maximum point total of 121,
it is highly unlikely to get into a situation where one player is half the board
behind the other.
%
While there have been anecdotes of losses by more than that amount,
it is a very rare occurrence and can be thought of as a failure of luck
rather than of skill.
%
As this is a matter of training skill and proficiency in the game of cribbage,
situations of exceptional bad luck can be treated as an outlier situation.
%%%

%\DeclareMathOperator{\variance}{
\newcommand{\Var}{\mathrm{Var}}
%%%
A further measure was taken to ensure adequate exploration.
%
Under normal conditions,
cards were selected by choosing the combination which had the highest value in
the produced \pvec\ vector.
%
At any given point in a training game, however,
there was a chance of selecting which cards are played
by random by using \pvec\ as a probability distribution.
%
This chance $e$ of random choice was related to the variance of the 
weights according to the formula:
\[
	e = k - \Var(\wvecm_{m,o,d})
\]
where $k$ is set at a constant $0.3$ empirically chosen during the
development process.
%
This was implemented in order to ensure that situations in which there were
more uniform weights had a higher chance of being explored
whereas those with a higher variance were deemed to be varied enough to
have been previously trained.
%%%

%%%
The training was intended to take the form of a tournament of learning in which
better and better agents would be paired off against each other until an
ultimate agent was reached,
allowing for more experience to be gained for agents from a variety of sources.
%
After a set number of training games,
in the case of this project one million,
the two agents were played against each other in a tournament match fashion.
%
The winner was determined as the agent with the most points at the end
of a series of games, %\textemdash
in this case 100, %\textemdash
wherein two points would be awarded to the winner of a game and three if that
win was by a margin of at least 31 points
and ties broken by total point spread,
according to ACC rules~\cite{ACC_rules}.
%
After the match had completed,
the winner would advance to the next round,
training against another winner from the previous round.
%
The process would be
repeated until one agent is declared the ultimate winner.
%%%

%%%
After the tournament structure was found to not provide any further benefits
to the learning process,
a set of additional experiments were performed to determine the limits of
learning possible.
%
While these will be covered in more detail in \ref{sec:findings},
these modifications include simple items such as learning rate or decay rate 
adjustments
as well as
more complicated adjustments
including using neighboring weights
or punishing a loss less severely than a corresponding win is rewarded
in addition to sanity checks by starting from pure strategies.
%%%

