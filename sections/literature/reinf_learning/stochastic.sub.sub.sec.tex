
%%%
% Section on applying machine learning to
%%%

\subsubsection*{Stochastic Games}

%%%
% Brief introduction of stochastic games
%%%

%%%
In contrast to the predictability of perfect information games,
there exist games in which the entire game state is not fully observable
or in which outcomes involve random chance.
%
These latter games,
in which state transitions
proceed according to a set of transition probabilities,
are called \textit{stochastic}~\cite{stochastic_games}.
%%%

%%%
% TD- Gammon
%%%

%%%
%Another pivotal paper in machine learning of games,
A pivotal example of machine learning applied to stochastic games,
and a heavy influence in AlphaGo's creation,
was TD-Gammon
\cite{tdgammon}.
%
In his paper,
Tesauro designed and trained a simple feed-forward neural network to play 
backgammon.
%
Backgammon is an ancient board game in which moving abilities are determined
through dice rolls.
%
The inclusion of dice rolls provides a stochastic environment since
a desired outcome may not be the actual result of making a move.
%
As a result,
the searching methodology of DeepBlue or Samuel's checkers is not applicable
as there is no guarantee of reaching a given state.
%
Instead,
the neural network is trained to predict the likely outcome of a game with a
given position
through observing several recorded games.
%
After a game had finished,
the graph's weights were readjusted by using a final reward signal representing
the actual outcome of the game.
%%%

%%%
Since predictions made later in the course of the game were expected to be more
accurate than those made earlier,
a system of diminished rewards was implemented,
called TD($\lambda$),
based on the idea of temporal difference.
%
TD($\lambda$) updates the weight differences according to the formula:
\[
	w_{t+1} - w_t =
		\alpha (Y_{t+1}-Y_t)
		\sum_{k=1}^t {\lambda^{t-k} \nabla_w Y_k}
\]
where $\alpha$ is a learning rate parameters,
$\nabla_w Y_k$ is the gradient of the output with respect to the weights,
and $\lambda$ controls how far back credit is assigned to an outcome
or more accurately how much that credit decays.
%
For instance,
$\lambda=0$ means only the last evaluation is credited,
while $\lambda=1$ means all evaluations are credited equally
and values in between provide smoothly decaying credit assignments.
%%%

%%%
Later experiments extended training through the use of self-play.
%
TD-Gammon would play both sides of a single game and adjust its evaluation
network accordingly.
%
This training method was proven to be highly successful when TD-Gammon was
entered into the 1992 World Cup of Backgammon tournament and was shown to be a
contender,
losing by only 7 points total over the course of 38 games with highly respected
players and even former world champions.
%
In qualitative evaluations by Kit Woolsey,
one of backgammon's most respected analysts,
TD-Gammon was declared to play better than humans in situations which were
considered more complex,
as its evaluation function was able to precisely
calculate the chances of winning without bias or emotion.
%
TD-Gammon has so impressed the backgammon community that strategies previously
considered inferior due to human bias have been revisited and reanalyzed due to
their favorability in TD-Gammon.
%%%

%%%
Tesauro credits this success to both the self-play method of training
as well as phenomena directly inherited from the nature of the game of
backgammon.
%
The motion of the game is reliant on the outcomes of random dice rolls;
the nature of the outcomes of these rolls naturally contributes to the
exploration of a wide search space without the need for explicit exploration
steps as would be needed in a perfect information game such as chess.
%
Furthermore,
since checkers can only be moved forward
with the exception of being knocked back by an advancing opponent,
the game cannot enter a non-terminating loop and must eventually
terminate.
%%%

%%%
In addition to the backgammon community,
TD-gammon made leaps and bounds in the area of games AI research,
especially with respect to neural networks.
%
After the first few thousand training games,
the system was capable of playing elementary backgammon,
recognizing a few basic strategies for decent play.
%
This was determined to be the result of simple combinations of features which
could be quickly evaluated by a linear function of the raw board position.
%
More complicated, context-sensitive strategies would develop later in training
and are credited as being the result of the ability of neural networks to adapt
to nonlinear features.
%%%


%%%
While Tesauro credits the neural network architecture and the game itself for
the success of the TD-Gammon system,
Drs.\ Pollack and Blair from Brandeis University undertook to demonstrate that
TD-Gammon's successes were not a result of
temporal difference learning
or even reinforcement learning necessarily,
but due to the nature of backgammon itself and how self-play contributes to a
constantly evolving environment
\cite{why-td-gammon}.
%
In their efforts,
Pollack and Blair applied basic hill-climbing techniques to a simple neural
network setup similar to that described in Tesauro's paper.
%
To this network,
the authors applied the same strategy as Samuel's generalized learning:
a mutant would be created by adding Gaussian noise to the ``champion'' weights
and the normal weights would play the mutant for a number of games.
%
These games were done in pairs with each agent getting the chance to play as
white and the same random seed to avoid unfair bias in the dice rolls.
%
With this setup,
if two agents are identical,
then the result of two games would be one win each.
%
If the mutant then won enough games,
the champion's weights would then be shifted in the direction of the mutant.
%
At no point was reinforcement or temporal difference credit used to adjust the
neural network's weights;
not even backpropagation was used.
%%%

%%%
The authors were able to perform similarly to an earlier version of TD-Gammon,
based on comparative performance against other backgammon-playing systems.
%
This led the authors to the conclusion that the success of TD-Gammon was
more likely
the result of the dynamics of backgammon and how they contribute to providing an
excellent environment for self-play learning,
not necessarily a result of the technique used in optimizing the neural network.
%
In normal student-teacher learning,
a teacher will attempt to point out edge cases of the student's knowledge
and the student in turn learns by asking tougher and tougher questions.
%
In self-play learning,
the teacher and student are the same,
so it is possible that a student and teacher can reach a draw situation in which
both agents are placated and neither is challenged to further best the other.
%
Characteristics mentioned by the authors include the lack of ability for a game
to end in a draw, which, in turn, means that either the teacher or student is
always required to improve its performance,
leading to continuous improvement.
%%%

