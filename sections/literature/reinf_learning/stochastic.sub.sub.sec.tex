
%%%
% Section on applying machine learning to
%%%

\subsubsection*{Stochastic Games}

%%%
% Brief introduction of stochastic games
%%%

%%%
In contrast to the predictability of perfect information games,
there exist games in which the entire game state is not fully observable
or in which outcomes involve random chance.
%
These latter games,
in which state transitions
proceed according to a set of transition probabilities,
are called \textit{stochastic}~\cite{stochastic_games}.
%
Examples include games involving
dice rolls (e.g.\ backgammon, craps),
playing cards (e.g.\ poker, blackjack, cribbage),
and other games of change (e.g.\ roulette, slot machines).
%%%

%%%
% TD- Gammon
%%%

%%%
%Another pivotal paper in machine learning of games,
A pivotal example of machine learning applied to stochastic games,
and a heavy influence in AlphaGo's creation,
was TD-Gammon
\cite{tdgammon}.
%
In his paper,
Tesauro designed and trained a simple feed-forward multilayer perceptron to play 
backgammon.
%
Backgammon is an ancient board game in which moving abilities are determined
through dice rolls.
%
The inclusion of dice rolls provides a stochastic environment since
a desired move may or may not be possible given a dice roll.
%
As a result,
the searching methodology of DeepBlue or Samuel's checkers is not applicable
as there is no guarantee of reaching a given state.
%
Instead,
the neural network was trained to predict the likely outcome of a game from a
given position
through observing several recorded games.
%
After a game had finished,
the graph's weights were readjusted by using a final reward signal representing
the actual outcome of the game.
%%%

%%%
Since predictions made later in the course of the game were expected to be more
accurate than those made earlier,
a system of diminished rewards was implemented,
called TD($\lambda$),
based on the idea of temporal difference.
%
TD($\lambda$) updates the weight differences according to the formula:
\[
	w_{t+1} - w_t =
		\alpha (Y_{t+1}-Y_t)
		\sum_{k=1}^t {\lambda^{t-k} \nabla_w Y_k}
\]
where $\alpha$ is a learning rate parameter,
$\nabla_w Y_k$ is the gradient of the output with respect to the weights,
and $\lambda$ controls how far back credit is assigned to an outcome
or more accurately how much that credit decays.
%
For instance,
$\lambda=0$ means only the last evaluation is credited,
while $\lambda=1$ means all evaluations are credited equally
and values in between provide smoothly decaying credit assignments.
%%%

%%%
Later experiments extended training through the use of self-play.
%
TD-Gammon would play both sides of a single game and adjust its
weights according to the observed result.
%
This training method was proven to be highly successful when TD-Gammon was
entered into the 1992 World Cup of Backgammon tournament and was shown to be a
contender,
losing by only 7 points total over the course of 38 games with highly respected
players and even former world champions.
%
In qualitative evaluations by Kit Woolsey,
one of backgammon's most respected analysts,
TD-Gammon was declared to play better than humans in situations which were
considered more complex,
as its evaluation function was able to precisely
calculate the chances of winning without bias or emotion.
%
TD-Gammon so impressed the backgammon community that strategies previously
considered inferior due to human bias have been revisited and reanalyzed due to
their favorability in TD-Gammon.
%%%

%%%
Tesauro credits this success to both the self-play method of training
as well as phenomena directly inherited from the nature of the game of
backgammon.
%
The motion of the game is reliant on the outcomes of random dice rolls;
the nature of the outcomes of these rolls naturally contributes to the
exploration of a wide search space without the need for explicit exploration
steps as would be needed in a perfect information game such as chess.
%
Furthermore,
since checkers can only be moved forward
with the exception of being knocked back by an advancing opponent,
the game cannot enter a non-terminating loop and must eventually
terminate.
%%%

%%%
In addition to the backgammon community,
TD-Gammon made leaps and bounds in the area of games AI research,
especially with respect to neural networks.
%
After the first few thousand training games,
the system was capable of playing elementary backgammon,
recognizing a few basic strategies for decent play.
%
This was determined to be the result of simple combinations of features which
could be quickly evaluated by a linear function of the raw board position.
%
More complicated, context-sensitive strategies would develop later in training
and are credited as being the result of the ability of neural networks to detect
nonlinear features.
%%%


%%%
While Tesauro credits the neural network architecture and the game itself for
the success of TD-Gammon,
Drs.\ Pollack and Blair from Brandeis University demonstrated that
TD-Gammon's successes were not a result of
temporal difference learning
or even reinforcement learning necessarily,
but due to the nature of backgammon itself and how self-play contributes to a
constantly evolving environment
\cite{why-td-gammon}.
%
In their efforts,
Pollack and Blair applied basic hill-climbing techniques to a simple neural
network setup similar to that described in Tesauro's paper.
%
To this network,
the authors applied a similar strategy to Samuel's generalized learning:
a mutant was created by adding Gaussian noise to the ``champion'' weights.
%
The normal weights played the mutant for a number of games.
%
Each of these games was played in pairs
so each agent got the chance to play as white,
and started with the same random seed to avoid unfair bias in the dice rolls.
%
With this setup,
if two agents are identical,
then the result of the two games would be one win each.
%
If the mutant then won enough games,
the champion's weights were shifted in the direction of the mutant.
%
At no point was reinforcement or temporal difference credit used to adjust the
neural network's weights.
%%%

%%%
The authors were able to perform similarly to an earlier version of TD-Gammon,
based on comparative performance against other backgammon-playing systems.
%
This led the authors to the conclusion that the success of TD-Gammon was
more likely
the result of the dynamics of backgammon and how they contribute to providing an
excellent environment for self-play learning,
not necessarily a result of the technique used in optimizing the neural network.
%
In normal student-teacher learning,
a teacher will attempt to point out edge cases of the student's knowledge
and the student, in turn, learns by asking tougher and tougher questions.
%
In self-play learning,
the teacher and student are the same,
so it is possible that a student and teacher can reach a draw situation in which
both sides are placated and neither is challenged to further best the other.
%
Backgammon, however,
cannot end in a draw,
so one of the players is always forced to improve.
%%%

