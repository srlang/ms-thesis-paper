
%%%
% Discussion of DeepMind's AlphaGo strategy and DeepBlue's Chess
%%%

\subsubsection*{Perfect Information Games}

%%%
Perfect information games are those in which the entire game state is observable
at any given time~\cite{perfinfo}.
%
Furthermore,
an action has a definite and entirely predictable outcome:
e.g. moving a chess piece on the board in a specific arrangement will always
lead to the same resulting arrangement.
%
Checkers, chess, and Go are all examples of perfect information games.
%%%

%%%
Jumping ahead by nearly 40 years,
the basic ideas of Samuel's paper can be seen applied to the slightly different
domain of chess with IBM's DeepBlue
and its victory against Garry Kasparov.
%
The DeepBlue team greatly expanded upon the idea of a minimax search to create a
massively parallel search system for chess evaluation
\cite{ibm_deepblue}.
%
The final system was able to evaluate hundreds of millions of chess positions
per second,
millions of times faster than Samuel's capabilities at the time.
%
However,
the speed in evaluation was a result of hard-wiring an evaluation function at
the chip level using chips custom-built for the purpose.
%
As with rote learning,
very little in terms of actual learning how to play or adjustments to internal
mechanisms occurred
beyond an expanded search space made possible by more capable hardware.
%
Despite this philosophical gripe,
the IBM team was able to outplay a Grandmaster of chess,
winning the match 3.5 games to 2.5.
%%%

%%%
% deepmind's alphago
%%%

%%%
Of notable attention recently is Google DeepMind's
success in the game of Go with AlphaGo
\cite{deepmind_alphago}.
%
More satisfying than DeepBlue's search approach,
AlphaGo used multiple neural networks to evaluate value and policy functions.
%
A first neural network was trained using a database of human-played games
to predict human play for a given board position.
%
Using this network as a starting point,
a policy network was trained to win more games than its previous renditions.
%
From that policy network,
a value network was trained to determine the likelihood of winning when in a
given state.
%
Both of these previous networks were trained through self-play.
%
The AlphaGo program would use this value network in combination with a
Monte Carlo Tree Search algorithm,
similar in nature to a minimax tree
but using Monte Carlo simulations to create approximate state values within the
tree as well as a policy to determine which transitions to take within the
simulation tree,
to determine the optimal move to make next.
%
The final value network resulting from this training pipeline was able to beat
European Go Champion Fan Hui in match play
five games to zero~\cite{deepmind_alphago},
eventually going as far as winning against
Grandmaster Lee Sedol four games to one~\cite{deepmind_overview}.
%%%

%%%
DeepMind then took a step further,
removing all human-imposed knowledge and
training a single neural network to play \textit{tabula rasa}, i.e. from scratch.
%
Using only this single neural network,
the team trained AlphaGo Zero,
the new program,
using a simplified Monte Carlo tree search algorithm to evaluate positions
during the game.
%
After a self-play game had been played,
the network used batches of observed states during the game to update
the network's weights to predict values
more closely align to those of the simulated policy.
%
After only 72 hours of training and using only a single machine,
AlphaGo Zero was able to defeat the previous version of AlphaGo
one hundred games to zero~\cite{deepmind_alphago_zero}.
%
For context,
AlphaGo was distributed across multiple machines
and trained over a period of several months.
%%%

