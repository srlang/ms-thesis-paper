
%%%
% Discussion of DeepMind's AlphaGo strategy and DeepBlue's Chess
%%%

\subsubsection*{Perfect Information Games}

%%%
Perfect information games are those in which the entire game state is observable
at any given time~\cite{perfinfo}.
%
Furthermore,
an action has a definite and entirely predictable outcome:
e.g.\  moving a chess piece on the board in a specific arrangement will always
lead to the same resulting arrangement.
%
Checkers, chess, and Go are all examples of perfect information games.
%%%

%%%
Jumping ahead by nearly 40 years,
the basic ideas of Samuel's paper can be seen applied to the slightly different
domain of chess with IBM's DeepBlue
and its victory against Garry Kasparov.
%
The DeepBlue team greatly expanded upon the idea of a minimax search to create a
massively parallel search system for chess evaluation
\cite{ibm_deepblue}.
%
The final system was able to evaluate hundreds of millions of chess positions
per second,
millions of times faster than Samuel's capabilities at the time.
%
However,
the speed in evaluation was a result of hard-wiring an evaluation function
by using chips custom-built for the purpose.
%
As with Samuel's rote learning,
very little in terms of actual learning how to play or adjustments to internal
mechanisms occurred
beyond an expanded search space made possible by more capable hardware.
%
Despite this philosophical gripe,
the IBM team was able to outplay a Grandmaster of chess,
winning the match 3.5 games to 2.5.
%%%

%%%
% deepmind's alphago
%%%

%%%
Of notable attention recently is Google DeepMind's
success in the game of Go with AlphaGo
\cite{deepmind_alphago}.
%
More of a learning solution than DeepBlue's engineering solution,
AlphaGo used multiple neural networks to evaluate value and policy functions.
%
A first neural network was trained via supervised learning,
using a database of human-played games
to predict human play for a given board position.
%
Using this network as a starting point,
a policy network was trained via reinforcement learning
to win more games than its previous renditions.
%
From that policy network,
a value network was trained to determine the likelihood of winning when in a
given state.
%
Both of these previous networks were trained through self-play.
%
The AlphaGo program would use both the value and policy networks
in a lookahead search algorithm
to determine the optimal move to make next.
%
Potential game outcomes were searched in a similar fashion to minimax,
but using the policy network to determine which move to make at any state,
rather than evaluating all possible legal moves and pruning.
%
Starting from the current state,
possible games were simulated by using the policy network,
expanding each potential outcome in the tree by using the value network
and deeper simulations,
also incorporating exploration mechanisms.
%
After all searches reached a terminal state,
the most commonly taken action from the starting position was selected
as the agent's move,
as it represents the choice which would optimize the expected outcome of
simulations using all combined knowledge.
% TODO: ^^^ AlphaGo explanation.
%
The final system resulting from this training pipeline was able to beat
European Go Champion Fan Hui in match play
five games to zero~\cite{deepmind_alphago}.
%
A more trained version of AlphaGo would eventually beat
Grandmaster Lee Sedol four games to one~\cite{deepmind_overview}.
%%%

%%%
DeepMind then took a step even further,
removing all human-imposed knowledge and
training a single neural network to play \textit{tabula rasa}, i.e.\  from scratch.
%
Using only this single neural network,
the team trained AlphaGo Zero,
the new program,
using a simplified Monte Carlo tree search algorithm to evaluate positions
during the game.
%
To train the network to predict values
more closely aligned with those of the simulated policy,
after a self-play game had been played,
states observed during the game were used to update
the network's weights.
%
After only 72 hours of training and using only a single machine,
AlphaGo Zero was able to defeat the previous version of AlphaGo
one hundred games to zero~\cite{deepmind_alphago_zero}.
%
For context,
AlphaGo was distributed across multiple machines
and trained over a period of several months.
%%%

