
\subsection{Prior Cribbage Research}

%%%
% Prior research performed in cribbage
%	paper on dealer's advantage
%	Evolutionary/genetic card picker
%	O'Connor's MLP
%	whatever else found during proposal time
%%%

%%%
On the topic of cribbage, very little research has been done,
perhaps as a result of its relative lack of popularity to such games as poker
or its stochastic environment being too large to tackle.
%
There are three main papers in which cribbage receives the main focus:
a mathematical analysis of dealer advantage~\cite{cribbage_optimal_ev},
a genetic learner applied to the discard phase~\cite{adaptive_cribbage},
and a multilayer perceptron attempt to apply a simpler version of TD($\lambda$)
to cribbage~\cite{roconnor_cs486}.
%%%

%%%
In his senior thesis,
Philip Martin set out to find if the player which begins as the dealer
had any statistical advantage in winning
\cite{cribbage_optimal_ev}.
%
The method used to accomplish this task was to enumerate and evaluate all
possible combinations of cards which can be dealt to a player.
%
From there,
a matrix of potential average crib scores was created as a table indexed on each
axis by one of ${52 \choose 2}$ combinations,
introducing the minor accuracy of forgetting which cards have been seen by each
player from their respective hands.
%
Using the bounds found,
certain basic strategies were proven to be suboptimal,
e.g. avoiding throwing a pair or fifteen to the crib as the pone,
since at least one of their expected outcomes
was found to be below the lower bound for optimal strategies.
%
Despite the slight inaccuracy and assumption of a uniform distribution,
the paper did conclude that being the first player to act as dealer gave the
player an expected advantage of approximately 5 points on average
as a result of starting with a guaranteed count of a crib.
%
However,
since different strategies needed for other portions of the game were not
mentioned,
by the author's own admission, this study barely scratched the surface of
research into the game.
%%%

%%%
The next,
and perhaps most useful,
study performed on the game of cribbage is Robert O'Connor's undergraduate
project
\cite{roconnor_cs486}
on adapting TD($\lambda$)~\cite{tdgammon} to the domain of cribbage.
%
O'Connor trained a feedforward multilayer perceptron with only a single hidden
layer to play a single hand of cribbage.
%
He used TD($\lambda$) to adjust the weights as training proceeded with the
network playing full games against itself:
no score context data was included in the input vector for training.
%
After training for millions of games,
the agent was able to choose hands reliably better than random,
but just shy of an algorithm which would choose by maximum expected outcome.
%
As there was no previous work done to produce an AI to play cribbage,
no comparison could be made.
%
Furthermore,
as the results of games vary wildly due to luck of card games,
an infeasible amount of games would need to be played against a human to
determine if any advantage was present,
and thus a comparison was not made to human play either.
%
Regardless,
the system was demonstrated to be learning as it played significantly
better than random over the course of matches of one thousand games each.
%%%

%%%
It should be noted that the paper lacks significant details
as to how the network was used, precisely.
%
The output of the network was a single linear output representing the state
utility function at any state.
%
These states were represented by 209 binary values representing various cards in
hand or in play.
%
There is no mention of how this state space is explored or how this value
function is applied to the agent making a decision;
it can only be assumed that the agent made the greedy choice among potentially
reachable states for each possible legal move.
%%%

%%%
The final paper worth addressing which attempts to learn cribbage
is Kendall's and Shaw's adaptive cribbage player
\cite{adaptive_cribbage}.
%
The authors use an evolutionary strategy,
similar to a genetic algorithm,
to make an agent learn to play the discard phase of the game.
%
In doing so,
the limitation was imposed that the suit of a card would not be considered,
sacrificing a small degree of accuracy in calculations to vastly simplify the
search space.
%
For each hand dealt,
the evolutionary algorithm attempted to learn a set of weights for choosing
which cards to keep and which to throw into the crib,
in order to optimize its own points gained.
%
The choices made were contrasted against optimal possible choices for that hand
when including the cut card in order to determine fitness
and weights were updated if the resulting hand scored below a threshold
percentage of optimal.
%
While initial results were positive and learning occurred alongside an increase
in points obtained by the agents,
performance eventually returned to levels on par with the initial randomized
weights.
%
This was because values associated with each card converged until suboptimal
behavior was reintroduced.
%%%

%%%
Additionally,
the authors attempted also to use a co-evolutionary strategy in which
two agents would learn to adjust their weights for each hand when playing each
other by using the opponent's score as comparison for weights adjustment.
%
This proved to be detrimental to agent performance when different hands were
dealt to each competing agent
as a result of the fact that one agent must always update its weights
as the loser
and because losing may be unfair since the values of hands can vary widely.
%
However,
when the agents were trained by using the same set of cards,
results were far more positive since the losing agent was always challenged to
improve on its present choice in a fair manner.
%%%

%%%
The paper continued by prompting the evolutionary agent to learn a different set
of weights for the hand
when playing as either the dealer or as the pone.
%
As a demonstration,
a single set of dealt cards was set to the task
and different sets of weights were learned for each position.
%%%

%%%
As a final experiment,
the trained agents were played against a commercially available cribbage-playing
program on various difficulty setting levels.
%
To supplement the developed card-choosing agent,
a simple heuristic was included to play the pegging portion of the game.
%
This conglomerate agent was able to outperform the opponent program
easily on its `easy' difficulty.
%
The matches were more evenly matched between the agent and the program on
its `medium' difficulty,
narrowly winning three of five games.
%
The `hard' and `harder' difficulties proved too much for the simple pegging
heuristic
and the agent lost all five games.
%%%

%%%
Thus,
the authors have demonstrated that an agent can be made to learn how to choose a
combination of cards well using evolutionary methods.
%
Furthermore,
they have demonstrated that the agent could be trained to recognize differences
in play present when playing as the dealer or as the pone.
%%%

